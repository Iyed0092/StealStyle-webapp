{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8fab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MY GPU IS TOO WEAK FOR THIS :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba70ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\iyedm\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python310.zip', 'c:\\\\Users\\\\iyedm\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\DLLs', 'c:\\\\Users\\\\iyedm\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib', 'c:\\\\Users\\\\iyedm\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310', '', 'C:\\\\Users\\\\iyedm\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages', 'C:\\\\Users\\\\iyedm\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\win32', 'C:\\\\Users\\\\iyedm\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\iyedm\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\iyedm\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402b59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\iyedm\\OneDrive\\Desktop\\NST\\backend\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5a993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from hybrid_text.data.load_data import ProseDataset\n",
    "from hybrid_text.models.hybrid_model import HybridTextModel\n",
    "from hybrid_text.utils import set_seed, save_checkpoint\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from hybrid_text.data.load_data import ProseDataset\n",
    "from hybrid_text.models.hybrid_model import HybridTextModel\n",
    "from hybrid_text.utils import set_seed, save_checkpoint\n",
    "from hybrid_text.data.preprocess import decoder_tokenizer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e1e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['id', 'translated_dialog', 'og_response']\n",
      "Available columns: ['id', 'translated_dialog', 'og_response']\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 3\n",
    "LR_TRANS = 3e-5\n",
    "LR_GAN = 1e-4\n",
    "MAX_LENGTH = 128\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "train_ds = ProseDataset(split=\"train\", max_length=MAX_LENGTH)\n",
    "val_ds = ProseDataset(split=\"test\", max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133c74cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training examples: 5273, Validation examples: 3516\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: ProseDataset.collate_fn(b, MAX_LENGTH))\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda b: ProseDataset.collate_fn(b, MAX_LENGTH))\n",
    "\n",
    "print(f\"[INFO] Training examples: {len(train_ds)}, Validation examples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae4ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = HybridTextModel(device=DEVICE).to(DEVICE)\n",
    "\n",
    "# Separate optimizer params\n",
    "params_trans = list(model.encoder.model.parameters()) + list(model.decoder.model.parameters()) + list(model.decoder.proj.parameters())\n",
    "params_gan = list(model.generator.parameters()) + list(model.discriminator.parameters())\n",
    "\n",
    "optimizer_trans = torch.optim.Adam(params_trans, lr=LR_TRANS)\n",
    "optimizer_gan = torch.optim.Adam(params_gan, lr=LR_GAN)\n",
    "\n",
    "# Losses\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ee41cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iyedm\\AppData\\Local\\Temp\\ipykernel_18260\\3649950587.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # for AMP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Epoch 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/528 [00:00<?, ?it/s]C:\\Users\\iyedm\\AppData\\Local\\Temp\\ipykernel_18260\\3649950587.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 47190 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n",
      "Batch shapes: torch.Size([10, 128]) torch.Size([10, 128]) torch.Size([10, 128])\n",
      "lm_logits: torch.Size([10, 129, 250054]) refined: torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 1/528 [00:08<1:17:48,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 44761 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 2/528 [00:15<1:04:38,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 46199 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 3/528 [00:21<1:01:29,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 45676 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 4/528 [00:28<1:01:55,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 48586 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 5/528 [00:34<57:15,  6.57s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 44761 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 6/528 [00:40<54:33,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 44761 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▏         | 7/528 [00:47<55:48,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 48126 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|▏         | 8/528 [00:57<1:05:38,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 46199 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|▏         | 9/528 [01:05<1:08:30,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch shapes ---\n",
      "Encoder input_ids: torch.Size([10, 128])\n",
      "Encoder attention_mask: torch.Size([10, 128])\n",
      "Decoder input_ids: torch.Size([10, 128])\n",
      "Decoder attention_mask: torch.Size([10, 128])\n",
      "Labels: torch.Size([10, 128])\n",
      "--- Token ID sanity ---\n",
      "Encoder max ID: 44761 Vocab size: 50265\n",
      "Decoder max ID: 250004 Vocab size: 250054\n",
      "Encoder pad token: 1\n",
      "Decoder pad token: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:824: UserWarning: Error detected in NllLossBackward0. Traceback of forward call that caused the error:\n",
      "  File \"c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\iyedm\\AppData\\Local\\Temp\\ipykernel_18260\\3649950587.py\", line 57, in <module>\n",
      "    loss_ce = ce_loss(logits_trim.view(-1, logits_trim.size(-1)), labels.view(-1))\n",
      "  File \"c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1297, in forward\n",
      "    return F.cross_entropy(\n",
      "  File \"c:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py\", line 3494, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:127.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "                                                          \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 612.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.46 GiB is allocated by PyTorch, and 513.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_ce \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m loss_g\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(total_loss) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(total_loss):\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mACCUM_STEPS\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN/Inf detected in total_loss at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, skipping update\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iyedm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 612.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.46 GiB is allocated by PyTorch, and 513.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm  # for nice progress bars\n",
    "\n",
    "# --- Config ---\n",
    "EPOCHS = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_GRAD_NORM = 1.0\n",
    "ACCUM_STEPS = 4  # gradient accumulation to simulate bigger batch\n",
    "LOG_EVERY = 10\n",
    "\n",
    "# Assume model, optimizer_trans, optimizer_gan, ce_loss, bce_loss, train_loader are defined\n",
    "scaler = GradScaler()  # for AMP\n",
    "torch.autograd.set_detect_anomaly(True)  # optional: helps catch in-place / NaN errors\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_ce, epoch_gan = 0.0, 0.0\n",
    "    step_counter = 0\n",
    "\n",
    "    print(f\"\\n=== Starting Epoch {epoch}/{EPOCHS} ===\")\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "\n",
    "    optimizer_trans.zero_grad()\n",
    "    optimizer_gan.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(progress_bar, 1):\n",
    "        enc_ids = batch[\"enc_input_ids\"].to(DEVICE)\n",
    "        enc_mask = batch[\"enc_attention_mask\"].to(DEVICE)\n",
    "        dec_ids = batch[\"dec_input_ids\"].to(DEVICE)\n",
    "        dec_mask = batch[\"dec_attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        # ===========================\n",
    "        # --- Forward pass ---\n",
    "        # ===========================\n",
    "        with autocast():\n",
    "            out = model(enc_ids, enc_mask, dec_ids, dec_mask, labels=labels, train_gan=True)\n",
    "            lm_logits = out[\"lm_logits\"]\n",
    "            refined = out[\"refined\"]\n",
    "\n",
    "        # Debug first batch only\n",
    "        if step == 1:\n",
    "            print(\"Batch shapes:\", enc_ids.shape, dec_ids.shape, labels.shape)\n",
    "            print(\"lm_logits:\", lm_logits.shape, \"refined:\", refined.shape)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Real embeddings for GAN\n",
    "        # ---------------------------\n",
    "        with torch.no_grad():\n",
    "            _, real_repr = model.encoder(enc_ids, enc_mask)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Cross-entropy loss\n",
    "        # ---------------------------\n",
    "        logits_trim = lm_logits[:, :labels.size(1), :].contiguous()\n",
    "        loss_ce = ce_loss(logits_trim.view(-1, logits_trim.size(-1)), labels.view(-1))\n",
    "\n",
    "        # ---------------------------\n",
    "        # GAN losses\n",
    "        # ---------------------------\n",
    "        if torch.isnan(refined).any() or torch.isinf(refined).any():\n",
    "            loss_g = torch.tensor(0.0, device=DEVICE)\n",
    "            loss_d = torch.tensor(0.0, device=DEVICE)\n",
    "        else:\n",
    "            fake_logits = model.discriminator(refined.float())\n",
    "            real_logits = model.discriminator(real_repr.detach().float())\n",
    "\n",
    "            loss_d = 0.5 * (bce_loss(real_logits, torch.ones_like(real_logits)) +\n",
    "                            bce_loss(fake_logits, torch.zeros_like(fake_logits)))\n",
    "            loss_g = bce_loss(fake_logits, torch.ones_like(fake_logits))\n",
    "\n",
    "        # ===========================\n",
    "        # Backward: discriminator\n",
    "        # ===========================\n",
    "        if loss_d.item() > 0:\n",
    "            scaler.scale(loss_d / ACCUM_STEPS).backward(retain_graph=True)\n",
    "\n",
    "        # ===========================\n",
    "        # Backward: generator + transformer\n",
    "        # ===========================\n",
    "        total_loss = loss_ce + 0.1 * loss_g\n",
    "        if not torch.isnan(total_loss) and not torch.isinf(total_loss):\n",
    "            scaler.scale(total_loss / ACCUM_STEPS).backward()\n",
    "        else:\n",
    "            print(f\"NaN/Inf detected in total_loss at step {step}, skipping update\")\n",
    "\n",
    "        # ===========================\n",
    "        # Optimizer step every ACCUM_STEPS\n",
    "        # ===========================\n",
    "        if step % ACCUM_STEPS == 0:\n",
    "            # Clip and step discriminator\n",
    "            torch.nn.utils.clip_grad_norm_(model.discriminator.parameters(), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer_gan)\n",
    "            scaler.update()\n",
    "            optimizer_gan.zero_grad()\n",
    "\n",
    "            # Clip and step transformer + generator\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer_trans)\n",
    "            scaler.update()\n",
    "            optimizer_trans.zero_grad()\n",
    "\n",
    "        # ===========================\n",
    "        # Logging\n",
    "        # ===========================\n",
    "        epoch_ce += loss_ce.item()\n",
    "        epoch_gan += loss_g.item() + loss_d.item()\n",
    "        step_counter += 1\n",
    "\n",
    "        if step % LOG_EVERY == 0:\n",
    "            progress_bar.set_postfix({\n",
    "                \"CE\": f\"{epoch_ce/step_counter:.4f}\",\n",
    "                \"GAN\": f\"{epoch_gan/step_counter:.4f}\",\n",
    "                \"step\": step\n",
    "            })\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Avg CE={epoch_ce/step_counter:.4f}, Avg GAN-term={epoch_gan/step_counter:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db11f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
